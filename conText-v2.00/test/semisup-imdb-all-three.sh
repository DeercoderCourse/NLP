  #####
  #####  IMDB: Training using three types of tv-embedding
  #####
  #####  Step 1. Generate input files.
  #####  Step 2. Training. 
  #####
  #####  NOTE1: To run this script, download unlab_data.tar.gz and decompress it at test/, 
  #####         so that the directory test/unlab_data will be created. 
  #####
  #####  NOTE2: For your convenience, the files of the results of tv-embedding 
  #####         learning (*.layer0) are provided.  They are endian sensitive 
  #####         and were generated with Little Endian (Intel convention). 
  #####         If they are not usable in your system, you need to generate them 
  #####         in your system using the provided scripts (see the comments 
  #####         on s_fn0, s_fn1, and s_fn2 below) and set sdir=output below.  
  #####

  gpu=-1          # <= Change this to, e.g., "gpu=0" to use a specific GPU.      
  sdir=unlab_data # <= Change this to the directory where tv-embedding files are. 
                  # <=   Provided at unlab_data/ (unlab_data.tar.gz). 
#  sdir=output    # <=   output/ if generated by semisup-imdb-{unsup|unsup3|parsup}-tv.sh.  

  dim=100 # dimensionality of tv-embeddings

  prep=../bin/prepText
  cnet=../bin/conText

  options="LowerCase UTF8"
  txt_ext=.txt.tok

  z=4 # to avoid name conflict with other scripts

  pch_sz=5
  s_fn0=${sdir}/imdb-uns-p${pch_sz}.dim${dim}.ite10.layer0       # generated by semisup-imdb-unsup-tv.sh
  s_fn1=${sdir}/imdb-parsup-p3p${pch_sz}.dim${dim}.ite10.layer0  # generated by semisup-imdb-parsup-tv.sh
  s_fn2=${sdir}/imdb-unsx3-p${pch_sz}.dim${dim}.ite10.layer0     # generated by semisup-imdb-unsup3-tv.sh

  #---  Step 1. Generate input files. 
  xvoc1=data/imdb${z}-trn.vocab
  $cnet $gpu write_word_mapping layer0_fn=$s_fn0 word_map_fn=$xvoc1  # extract word mapping from the tv-embedding file. 

  xvoc3=data/imdb${z}-trn-123gram.vocab  
  $cnet $gpu write_word_mapping layer0_fn=$s_fn2 word_map_fn=$xvoc3  # extract word mapping from the tv-embedding file. 

  for set in train test; do 
    opt=AllowZeroRegion
    #---  dataset#0: for the main layer (seq, same as semisup-imdb-{unsup|unsup3|parsup}-tv.sh)
    rnm=data/imdb${z}-${set}-p${pch_sz}seq
    $prep gen_regions $opt \
      region_fn_stem=$rnm input_fn=data/imdb-${set} vocab_fn=$xvoc1 \
      $options text_fn_ext=$txt_ext label_fn_ext=.cat \
      label_dic_fn=data/imdb_cat.dic \
      patch_size=$pch_sz patch_stride=1 padding=$((pch_sz-1))

    #---  dataset#1: for the side layer (bow, same as semisup-imdb-{unsup|parsup}-tv.sh)
    rnm=data/imdb${z}-${set}-p${pch_sz}bow
    $prep gen_regions $opt Bow \
      region_fn_stem=$rnm input_fn=data/imdb-${set} vocab_fn=$xvoc1 \
      $options text_fn_ext=$txt_ext RegionOnly \
      patch_size=$pch_sz patch_stride=1 padding=$((pch_sz-1))

    #---  dataset#2: for the side layer (bag-of-1-3grams, same as semisup-imdb-unsup3-tv.sh)
    rnm=data/imdb${z}-${set}-p${pch_sz}x3bow
    $prep gen_regions $opt Bow \
      region_fn_stem=$rnm input_fn=data/imdb-${set} vocab_fn=$xvoc3 \
      $options text_fn_ext=$txt_ext RegionOnly \
      patch_size=$pch_sz patch_stride=1 padding=$((pch_sz-1))
  done


  #---  Step 2. Training. 
  gpumem=${gpu}:4  # pre-allocate 4GB GPU memory. 

  logfn=log_output/imdb-semisup3-dim${dim}.log
  perffn=perf/imdb-semisup3-dim${dim}.csv
  echo 
  echo Supervised training using 3 types of tv-embedding to produce additional input.   
  echo This takes a while.  See $logfn and $perffn for progress and see param/semisup3.param for the rest of the parameters.
  $cnet $gpumem cnn 0side0_fn=$s_fn0 0side1_fn=$s_fn1 0side2_fn=$s_fn2 \
        trnname=imdb${z}-train-p${pch_sz} tstname=imdb${z}-test-p${pch_sz} \
        data_ext0=seq data_ext1=bow data_ext2=x3bow \
        0side0_dsno=1 0side1_dsno=1 0side2_dsno=2 \
        evaluation_fn=$perffn test_interval=25 LessVerbose \
        reg_L2=1e-4 step_size=0.1 \
        @param/semisup3.param > $logfn

