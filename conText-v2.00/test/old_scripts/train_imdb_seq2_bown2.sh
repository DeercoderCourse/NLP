  ####  Input: token file (one review per line; tokens are delimited by white space) 
  ####         label file (one label per line)
  ####  These input files were generated by prep_imdb.sh and included in the package. 
  ####  To find the order of the data points, see prep_imdb.sh and the files at lst/. 

  gpu=0  # <= change this to, e.g., "gpu=0" to use a specific GPU. 
  mem=4   # pre-allocate 4GB device memory 
  gpumem=${gpu}:${mem}


  prep_exe=../bin/prepText
  cnn_exe=../bin/conText

  #---  Step 1. Generate vocabulary for NB weights
  echo Generaing uni-, bi-, and tri-gram vocabulary from training data to make NB-weights ... 

  options="LowerCase UTF8"

  voc123=data/imdb_trn-123gram.vocab
  #---  Step 5. Training and test using GPU
  log_fn=log_output/imdb-seq2-bown.log
  perf_fn=perf/imdb-seq2-bown-perf.csv
  echo 
  echo Training CNN and testing ... 
  echo This takes a while.  See $log_fn and $perf_fn for progress and see param/seq2-bown.param for the rest of the parameters. 
  nodes0=20; nodes1=1000; nodes2=1000; nodes3=20 # number of neurons (weight vectors) in the convolution layers 
  $cnn_exe $gpumem cnn \
         0nodes=$nodes0 0resnorm_width=$nodes0 1nodes=$nodes1 1resnorm_width=$nodes1 2nodes=$nodes2 2resnorm_width=$nodes2 3nodes=$nodes3 3resnorm_width=$nodes3 \
         data_dir=data trnname=imdb_train- tstname=imdb_test- \
         data_ext0=nbw3 data_ext1=p2 data_ext2=p3 data_ext3=nbw6 \
         reg_L2=1e-4 step_size=0.25 top_dropout=0.5 \
         LessVerbose test_interval=25 evaluation_fn=$perf_fn \
         @param/seq2-bown-my.param > ${log_fn}
